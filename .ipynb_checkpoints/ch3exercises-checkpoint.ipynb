{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f243d274-6f45-4342-b121-3f0e6011153c",
   "metadata": {},
   "source": [
    "# Chapter 3 Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fec2b-7135-4ded-aee9-6630070a2f88",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ba76c-4132-4cae-9001-5ddd073fddb7",
   "metadata": {},
   "source": [
    "Describe the null hypotheses to which the $p$-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these $p$-values. Your explanation should be phrased in terms of `sales`, `TV`, `radio`, and `newspaper`, rather than in terms of the coefficients of the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8c83d-22b4-4f1d-bf2e-649e453e1167",
   "metadata": {},
   "source": [
    "|              | Coefficient | Std. error | t-statistic | p-value  |\n",
    "|--------------|-------------|------------|-------------|----------|\n",
    "| Intercept    | 2.939       | 0.3119     | 9.42        | < 0.00001 |\n",
    "| TV           | 0.046       | 0.0014     | 32.81       | < 0.00001 |\n",
    "| radio        | 0.189       | 0.0086     | 21.89       | < 0.00001 |\n",
    "| newspaper    | -0.001      | 0.0059     | -0.18       | 0.8599    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f58f6-8964-462a-a424-e9ceaa04ed81",
   "metadata": {},
   "source": [
    "> **Solution.** The $p$-values for `TV` and `Radio` correspond to $H_A$; that is, there is a relationship (in fact, a strong relationship) between `TV` and `sales`, and between `radio` and `sales`. With a low $p$ value for both, the chance of seeing the data under the assumption $H_0$ that the null hypothesis holds is low. The $p$-value for `newspaper` corresponds to the null hypothesis $H_0$ - being that there is no relationship between $X$ and $Y$. We can see this as a result of the high $p$-value for `newspaper`. **Q.E.D.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ec600-43ae-4af7-9274-79056aa7fb1d",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea27aba-9032-430a-b960-43779f897730",
   "metadata": {},
   "source": [
    "Carefully explain the differences between the KNN classifier and KNN regression methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c87be-5c00-4b30-9a5e-aea4e1b33d9a",
   "metadata": {},
   "source": [
    "> **Solution.** We first recall the KNN classifier and KNN regression methods:\n",
    "> * For the KNN classifer, given a positive $K$ and a test observation $x_0$, the KNN classifer identifies the $K$ points in the training data set that are closest to $x_0$, represented by $\\mathcal{N}_0$. It then estimates the conditional probability for class $j$ as the fraction of points in $\\mathcal{N}_0$ whose response values equal $j$: $$\\mathrm{Pr}(Y = j | X = x_0) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_0} I(y_i = j).$$ Finally, KNN classifies the test observation $x_0$ to the class with the largest probability.\n",
    "> * For the KNN regression method, it indentifies the $K$ training observations that are closest to $x_0$, represented by $\\mathcal{N}_0$, it then estimates $f(x_0)$ using the **average** of all the training responses in $\\mathcal{N}_0$. I.e., $$\\hat f(x_0) = \\frac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i.$$\n",
    "> A key difference between the KNN classifer and the KNN regression method, is that the KNN classifier assimilates the point of interest to the class with the largest probability, whereas the KNN regression method averages across a set of data. However, in a sense, the KNN classifier is producing an \"average\" being the expected class the given point should be a part of. For KNN regression, the optimal value of $K$ depends on the bias-variance tradeoff, whereas for the KNN classifer, we aim to choose $K$ to give a decision boundary close to the Bayes decision boundary. **Q.E.D.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6feac-cdc6-410a-86a9-9de901fbc2e3",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b361ed-e39d-4209-b93c-eda712d3a770",
   "metadata": {},
   "source": [
    "Suppose we have a data set with five predictors, $X_1 = \\mathrm{GPA}$, $X_2 = \\mathrm{IQ}$, $X_3 = \\mathrm{Level}$ (1 for College and 0 for High School), $X_4 = \\text{Interaction between GPA and IQ}$, and $X_5 = \\text{Interaction between GPA and Level}$. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\\hat \\beta_0 = 50, \\hat \\beta_1 = 20, \\hat \\beta_2 = 0.07, \\hat \\beta_3 = 35, \\hat \\beta_4 = 0.01, \\hat \\beta_5 = -10$.\n",
    "\n",
    "1) Which answer is correct, and why? i. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates. ii. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates. iii. For a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough. iv. For a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough\n",
    "2) Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n",
    "3) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2f155-10f2-4549-a02b-e155cda25536",
   "metadata": {},
   "source": [
    "> **Solution.**\n",
    "> 1. Statement (i) is incorrect, as if we look at the coefficient for level, $\\hat \\beta_3 = 35 > 0$, we see that for fixed $X_1$ and $X_2$ the response is lower if $X_3 = 0$. As (ii) is the converse of (i), it is true. (iii) is a boatload of cope. (iv) is misleading, as college graduates *generally* earn more than high school graduates according to this model.\n",
    "> 2. The salary of a college graduate with IQ of 110 and GPA of 4.0 is $$E[X] = 50 + 20 \\cdot 4.0 + 0.07 \\cdot 110 + 35 \\cdot 1 + 0.01 \\cdot 4.0 \\cdot 110 - 10 \\cdot 4.0 \\cdot 1 = 137.1 = \\$137,100.$$\n",
    "> 3. False. A small interaction coefficient does not necessarily suggest that there is no GPA/IQ interaction effect. We would need to see information about the $p$-value for $\\hat \\beta_4$ to better assess the presence of an interaction effect. **Q.E.D.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27074caf-e0c2-47d3-99fe-561f2fe73c6a",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd6acd-d5a1-4bed-ba52-adc3ef0f59fa",
   "metadata": {},
   "source": [
    "I collect a set of data ($n = 100$ observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e., $Y = \\beta_0 + \\beta_1 X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon$. \n",
    "1) Suppose that the true relationship between $X$ and $Y$ is linear, i.e. $Y = \\beta_0 + \\beta_1X + \\epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect to tell them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "2) Answer (1) using test rather than training RSS.\n",
    "3) Suppose that the true relationship between $X$ and $Y$ is not linear, but we don't know how far it is from linear. Consider the training RSS for the linear regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n",
    "4) Answer (3) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35903748-4915-44cc-98c6-f84b1ec67fb9",
   "metadata": {},
   "source": [
    "> **Solution.**\n",
    "> 1. Given that the true relationship between $X$ and $Y$ is linaer, we expect the RSS for cubic regression to be less than the RSS for linear regression. The linear regression is less flexible than the cubic one, and will not be able to interpolate through the training data as effectively as the cubic regression. \n",
    "> 2. However, using the **testing** data, the linear RSS will be lower than the cubic RSS. The cubic relationship, while more flexible, is more likely to overfit the data and track with noise, which affects the cubic regression's ability to demonstrate a linear trend. On the other hand, the linear regresion is more likely to approximate the true relationship (linear) better, as it is linear. \n",
    "> 3. Given that the true relationship is not linear, the cubic regression is still more flexible than the linear regression, and is likely to have a lower RSS than the linear regression for the training data.\n",
    "> 4. For the testing data, there may not be enough information to tell. If there is a degree-$2n$ relationship between $X$ and $Y$, then the linear and cubic regressions will both fail at giving a good approximation to the data. However, the flexibility of the cubic relationship allows for it to curve with the data, and at least from a local perspective, it is more likely to exhibit a lower RSS than the linear regression. **Q.E.D.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7fbad-7639-41ab-a50b-57d17d0b2b92",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeedcd2-ec87-43f2-9b08-51e5855fa581",
   "metadata": {},
   "source": [
    "Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $i$th fitted value takes the form $$\\hat y_i = x_i \\hat\\beta,$$ where $$\\hat \\beta = \\left(\\sum_{i = 1}^n x_i y_i\\right)/\\left(\\sum_{i' = 1}^n x_{i'}^2\\right).$$ Show that we can write $$\\hat y_i = \\sum_{i' = 1}^n a_{i'}y_{i'}.$$ What is $a_{i'}$? *Note*: We interpret this result by saying that the fitted values from linear regression are **linear combinations** of the response values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fd5af-767e-4b91-983e-2221340d32ca",
   "metadata": {},
   "source": [
    ">**Solution.** By direct computation, $$\\begin{align*} \\hat{y}_i &= x_i \\hat \\beta \\\\ &= x_i \\frac{\\sum_{i' = 1}^n x_{i'}y_{i'}}{\\sum_{j = 1}^n x_{j}^2} \\\\ &= \\sum_{i' = 1}^n \\frac{x_i x_{i'}}{\\sum_{j = 1}^n x_j} y_{i'}\\end{align*}$$ where we see that $a = \\frac{x_i x_{i'}}{\\sum_{j = 1}^n x_j}$. **Q.E.D.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306ddc1-1900-4398-b93a-04f4ed3e4fba",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44127515-4312-40af-a2cf-a1133811f3bb",
   "metadata": {},
   "source": [
    "## Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9526a0-1463-41ce-86e7-b1cfe8fb086b",
   "metadata": {},
   "source": [
    "## Exercise 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:islp]",
   "language": "python",
   "name": "conda-env-islp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
